---
title: "Rapid Fire Choice Analyses"
author: "Dave Braun"
date: "8/2/2019"
output: 
  html_document:
    code_folding: hide
    df_print: paged
    includes:
      after_body: ../../analysis/footer.html
      in_header: ../../analysis/favicon.html
knit:
  (function(inputFile, encoding) {
      rmarkdown::render(inputFile,
                    encoding = encoding,
                    output_file = 'index.html')})
---
```{r include = FALSE}
library(tidyverse)
library(data.table)
library(lme4)
library(ez)
```

*This document was last updated at `r Sys.time()`.*

This document is dedicated to the power analysis for Experiment 3. I'm taking a different approach here than I did for Experiments 1 and 2. Since Experiment 2 is so similar to this one, and I fit a GLMM to the data, which captures the fixed and random effects estimates from that data, I'll use simulations from that model to estimate the necessary sample for 0.8 power in Experiment 3.


```{r}
d <- read.csv('../../analysis/exp2/data/dstCleanChoice.csv')
d$selSafeDeck <- ifelse(d$selectedRiskyDeck == 1, 0, 1)
head(d)
```


## Fit the model to Experiment 2 data
I'm using the optimal model determined from [Experiment 2 Random Effects Analysis](../../analysis/exp2/scripts/exploratory/randomEffects/), which estimates a random intercept and two random slopes grouped by subject.

```{r}
d$differenceE <- ifelse(d$difference == 'Moderate', -0.5, 0.5)
d$difficultyE <- ifelse(d$difficulty == 'Easier than Reference', -0.5, 0.5)
m1 <- glmer(selSafeDeck ~ difficultyE * differenceE + (1 | subject) + (0 + difficultyE | subject) + (0 + differenceE | subject), data = d , family = binomial, control = glmerControl(optimizer = 'bobyqa'))
```


```{r}
options(warn=-1)
powerFun <- function(nRange, nSims) {
  
  result <- data.frame(N = numeric(), sim = numeric(), isSig = numeric())
  
  for (N in nRange) {
  
    print(paste('Subject: ', N, sep = ''))
    
    newdata <- createNewData(N)
     simObj <- simulate(m1, newdata = newdata, allow.new.levels = TRUE, nsim = nSims)
    condCodes <- data.frame(difficultyE = c(-0.5, -0.5, 0.5, 0.5), differenceE = c(-0.5, 0.5, -0.5, 0.5), difficulty = c(rep('Easier than Reference', 2), rep('Harder than Reference', 2)), difference = rep(c('Moderate', 'Extreme'), 2))
    newdata <- condCodes %>% 
      inner_join(newdata)
    newdata <- cbind(newdata, simObj)
    
    for (sim in 1:nSims) {
      curData <- newdata[,c('subject', 'difference', 'difficulty', paste('sim_', sim, sep = ''))]
      colnames(curData)[4] <- 'selSafeDeck'
      a1 <- ezANOVA(wid = subject, within = .(difference, difficulty), dv = selSafeDeck, data=curData, detailed = TRUE)
      result <- rbind(result, data.frame(N = N, sim = sim, isSig = a1$ANOVA$`p<.05`[3] == '*'))
    }

  }

  return(result)
  
}

```



```{r}
createNewData <- function(N) {
  newdata <- data.frame(subject = rep(1:N, each = 40), difficultyE = rep(rep(c(-0.5,0.5), each = 20, N)), differenceE = rep(rep(c(rep(-0.5, 10), rep(0.5, 10)), 2), N))
  return(newdata)
}
```



```{r warning = FALSE}
result <- powerFun(250, 50)
mean(result$isSig)
```

Power seems to be around .2 regardless of sample size. That doesn't make sense...


```{r warning = FALSE}

newdata <- expand.grid(subject = factor(1:200), differenceE = c(-0.5, 0.5), difficultyE = c(-0.5, 0.5))
nsim <- 50
beta <- unname(fixef(m1))
theta <- data.frame(VarCorr(m1))$sdcor

s <- simulate(~ difficultyE * differenceE + (1 | subject) + (0 + difficultyE | subject) + (0 + differenceE | subject), family = binomial, newdata = newdata, newparams = list(beta = beta, theta = theta), nsim = nsim, cond.sim = TRUE)                

N <- data.table(newdata)[,.(count = .N), by = subject][,.N]

condCodes <- data.frame(difficultyE = c(-0.5, -0.5, 0.5, 0.5), differenceE = c(-0.5, 0.5, -0.5, 0.5), difficulty = c(rep('Easier than Reference', 2), rep('Harder than Reference', 2)), difference = rep(c('Moderate', 'Extreme'), 2))

newdata <- condCodes %>% 
      inner_join(newdata)

newdata <- cbind(newdata, s)

result <- data.frame(N = numeric(), sim = numeric(), isSig = numeric())

for (sim in 1:nsim) {
  curdata <- newdata[, c('subject', 'difference', 'difficulty', paste('sim_', sim, sep = ''))]
  colnames(curdata)[4] <- 'selSafeDeck'
  a1 <- ezANOVA(wid = subject, within = .(difference, difficulty), dv = selSafeDeck, data=curData, detailed = TRUE)
  result <- rbind(result, data.frame(N = N, sim = sim, isSig = a1$ANOVA$`p<.05`[3] == '*'))
}
```




```{r}
curdata %>% 
  group_by(subject, difference, difficulty) %>% 
  summarize(selSafeDeck = mean(selSafeDeck)) %>% 
  group_by(difference, difficulty) %>% 
  summarize(ssd = mean(selSafeDeck), se = sd(selSafeDeck) / sqrt(N)) %>% 
  ggplot(aes(x = difficulty, y = ssd, group = difference)) +
  geom_bar(stat = 'identity', aes(fill = difference), position = position_dodge(width = .9)) +
  geom_errorbar(aes(ymin = ssd - se, ymax = ssd + se), position = position_dodge(width = .9), width = .5) +
  ylim(0,1)

curdata %>% 
  group_by(subject, difficulty) %>% 
  summarize(selSafeDeck = mean(selSafeDeck)) %>% 
  group_by(difficulty) %>% 
  summarize(ssd = mean(selSafeDeck), se = sd(selSafeDeck) / sqrt(N)) %>% 
  print() %>% 
  ggplot(aes(x = difficulty, y = ssd)) +
  geom_bar(stat = 'identity') +
  geom_errorbar(aes(ymin = ssd - se, ymax = ssd + se), width = .5) +
  ylim(0,1)

```


The data is different but signifance testing parameters remain exactly the same...

I was using this as a reference: https://rstudio-pubs-static.s3.amazonaws.com/11703_21d1c073558845e1b56ec921c6e0931e.html

Check this out: https://humburg.github.io/Power-Analysis/simr_power_analysis.html











