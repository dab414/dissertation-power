---
title: "Experiment 1 Power Summary"
author: "Dave Braun"
date: "March 15, 2019"
output: 
  html_document:
    df_print: paged
    code_folding: hide
---

Summarizing the power analysis for Experiment 1.  

Here is the final dataset generated by the power analysis:

```{r include = FALSE}
library(tidyverse)
library(DT)
library(ez)
d <- read.csv('../runSimulation/powerAnalysis.csv')
```

```{r}
datatable(d, filter = 'top', options = list(pageLength = 5, autoWidth = TRUE))
```


```{r}
d <- d %>% 
  filter(n == 100) %>% 
  select(bias, avgEffectSize) %>% 
  rename(effectCode = avgEffectSize) %>% 
  inner_join(d) 
```

## Logic Behind the Analysis
This got somewhat complicated. But the core idea is rather simple.  

1. I programmed a model that produced the expected effects.
2. I manipulated over many simulations:  
    * The sample size (i.e., `n`)  
    * The size of the expected effects (i.e., `bias`)  
3. Each parameter combination was simulated as an experiment 50 times.

There were two main scripts involved in running these simulations (click names of scripts to see them):  

  * [`powerSimulator.r`](https://github.com/dab414/dissertation-power/blob/master/exp1/runSimulation/powerSimulator.r){target='_blank'} : The script used to run simulations of experiments.  
  * [`hyperparameterTuning.r`](https://github.com/dab414/dissertation-power/blob/master/exp1/runSimulation/hyperparameterTuning.r){target='_blank'} : The script specifying the hyperparameters [sample size (`n`), effect size (`bias`)] to be used in the `powerSimulator.r` script.  

### Assumptions about Variance
I also made some assumptions about the sources of variability throughout all simulated experiments. Because I'm expecting a large degree of between-subject variability in this experiment -- and because the inferences to be made from this experiment have rather strict assumptions about this variability -- I thought it important to add this component into the simulations. This part gets a little complicated, but I essentially reverse-engineered random effects, where I defined the degree of variability in the fixed effects across subjects.  

These assumptions about noise variability are admittedly a bit arbitrary, but I tried to adjust the levels of the noise parameters prior to doing the main simulations to make sure they were within the range of what at least seemed reasonable.

The overall design of this experiment just involves one factor: (decoy: absent, near high demand, near low demand, neutral) predicting the proportion of low-demand selections. If you think about it in regression terms, the model that produces the decision is as follows:  

`choice = intercept + slope*decoyCondition + noise`  

Then, I introduce subject-level variability by using the following equations:  

`intercept = trueIntercept + subjectIntercept`  
`slope = trueSlope + subjectSlope`  

Each of these noise parameters [`noise`, `subjectIntercept`, `subjectSlope`] are sampled with a gaussian distribution with mean of 0 and standard deviation defined separately (and somewhat arbitrarily) for each term (see below). The 'subject' terms get sampled once per subject, and control how far that subject varies from the true population parameter. The `noise` parameter is sampled on each trial. The overall parameter I used to control the size of the true, expected effects is called `bias` (see above). The range of the bias and the standard deviations of each of the noise terms are defined below (on the log odds scale):  

```
bias = range(0.1 : 0.9)
subjectIntercept = 3
subjectSlope = 0.5
noise = 0.25
```
These levels are essentially saying that I expect much more variability in subject slope (`subjectIntercept = 3`; i.e., the extent to which they prefer the hard task over the easy task, or vice verse, at baseline) than I do in the between-subject variability in the size of the expected effect (`subjectSlope = 0.5`).  

I'm sure this is a bit confusing, and I'd be able to explain any part of the simulation process that you'd be interested in.  

These experiments are also assuming about 700 trials per subject within the last two phases of the experiment (i.e., two- and three-option choice phase). In retrospect, this estimate might be high--I'll know for sure after looking through the pilot data.

## Results of the Power Analysis

The results are visualized below.

```{r}
colnames(d)[4] <- 'Power'
d %>% 
  ggplot(aes(x = factor(n), y = factor(round(effectCode, 2)))) + geom_tile(aes(fill = Power), color = 'white') + scale_fill_gradient(low = 'steelblue', high = 'red') +
  xlab('Sample Size') + ylab('Effect Size') + labs(caption = 'Each tile represents the power to detect an effect across 50 simulated experiments.')
```

Based on these simulations, I should have full power to detect even a small effect (~ .1) with N = 40.  

We can look at the relationship between sample size and power for small effect sizes only (~.1 - ~.2):


```{r}
dLine <- d %>% 
  filter(avgEffectSize >= .1 & avgEffectSize <= .2) %>% 
  group_by(n) %>% 
  summarize(Power = mean(Power))

dLine %>% 
  ggplot(aes(x = n, y = Power)) +
  geom_hline(aes(yintercept = .8), linetype = 'dashed') +
  geom_line(size = 2) +
  xlab('Sample Size') +
  ylab('Power') +
  labs(caption = 'Effect Size between 0.1 and 0.2') +
  annotate('text', x = 75, y = .75, label = '0.8 Power') +
  annotate('text', x = 20, y = .87, label = '(n = 30, power = .81)') +
  theme_bw()
```

According to this result, to detect an effect somewhere between 0.1 and 0.2, we'd need a sample size of about 30. That seems rather small to me, and I'm putting 50 in the document now just to be safe.

### Sample Experiment Simulation
To give you a sense of what data from one experiment looks like, we can look at the data from one simulated experiment. This simulation will be at N = 50 and effect size ~ 0.1.

```{r results = 'hide', eval = FALSE}
source('../runSimulation/powerSimulator.r')
d <- powerSimulator(n = 50, bias = 0.3, nSims = 1)
write.csv(d, 'markdownSimulation.csv', row.names = FALSE)
```
```{r echo = FALSE}
d <- read.csv('markdownSimulation.csv')
m1 <- ezANOVA(wid = subject, within = condition, dv = lowDemandSelection, data = d, detailed = TRUE)
effectSize <- m1$ANOVA$SSn[2] / (m1$ANOVA$SSn[2] + m1$ANOVA$SSd[2])
pValue <- m1$ANOVA$p[2]
```


```{r}
results <- paste('F(', m1$ANOVA$DFn[2], ', ', m1$ANOVA$DFd[2], ') = ', round(m1$ANOVA$F[2], 2), ', p < .001', sep = '')
results <- paste(results, ', Effect Size = ', round(effectSize, 2), sep='')
```


```{r}
levels(d$condition) <- c('Neutral', 'Near High Demand', 'Near Low Demand', 'Absent')
d$condition <- factor(d$condition, levels(d$condition)[c(4, 2, 3, 1)])

d %>%
  #filter(condition == 'nearHigh' | condition == 'nearLow') %>% 
  group_by(subject, condition) %>%
  summarize(lowDemandSelection = mean(lowDemandSelection)) %>%
  group_by(condition) %>%
  summarize(lds = mean(lowDemandSelection), se = sd(lowDemandSelection) / sqrt(n())) %>%
  ggplot(aes(x = condition, y = lds)) + geom_bar(stat = 'identity', width = .7) + 
  geom_errorbar(aes(ymin = lds - se, ymax = lds + se), width = 0.5) +
  ylim(0,1) +
  xlab('Decoy') + 
  ylab('Proportion of Low-Demand Selections') + 
  theme_bw() #+
  # theme(panel.grid.major = element_blank(), #panel.grid.minor = element_blank(),
  #       axis.text.x = element_blank(),
  #       axis.ticks = element_blank(),
  #       axis.title.y = element_blank())

#ggsave('barPlot.png', width = 5.5, height = 3.5, units = 'in')


## subject lines
d %>% 
 # filter(condition == 'nearHigh' | condition == 'nearLow') %>% 
  filter(subject %in% 1:50) %>% 
  group_by(subject, condition) %>% 
  summarize(lds = mean(lowDemandSelection)) %>% 
  ggplot(aes(x = condition, y = lds)) + #geom_boxplot(width = .5, alpha = .5, fatten = NULL) +
  #stat_summary(fun.y = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..),
              # width = 0.5, size = 1, linetype = "solid") +
  geom_line(aes(group = subject)) +
  xlab('Decoy') + 
  ylab('Proportion of Low-Demand Selections') + 
  ylim(0,1) +
  labs(caption = 'Each line represents one subject') +
  theme_bw() #+
  # theme(panel.grid.major = element_blank(), #panel.grid.minor = element_blank(),
  #       axis.text.x = element_blank(),
  #       axis.ticks = element_blank(),
  #       axis.title.y = element_blank())

#ggsave('boxPlot.png', width = 5.5, height = 3.5, units = 'in')
```

This looks pretty messy, but I think the variability is representative of what we'll see in the actual data.

In this particular experiment, we managed to find the effect: `r results`

## Final Thoughts

Overall, the power analysis suggests that I need fewer subjects (N = ~30-40) than I would've guessed. I might rerun this analysis once I have a better idea (from the pilot data) of how many trials we'll be getting per subject. Right now, these analyses assume about 700 trials per subject, which is probably a bit high.




